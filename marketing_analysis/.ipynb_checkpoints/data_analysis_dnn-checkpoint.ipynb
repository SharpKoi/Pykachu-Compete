{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.utils import plot_model\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, MaxPooling1D, Embedding, Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCABULARY_NUM = 62000\n",
    "INPUT_SEQUENCE_LENGTH = 1600\n",
    "EMBEDDING_DIM = 100\n",
    "LABEL_NUM = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanedsrc = 'data/cleaned_data/'\n",
    "src = 'data/train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>137165</th>\n",
       "      <td>137165</td>\n",
       "      <td>very good value for money like most districts ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30633</th>\n",
       "      <td>30633</td>\n",
       "      <td>good product quality product quality good pric...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118020</th>\n",
       "      <td>118020</td>\n",
       "      <td>packaging containers once with bubble wrap fas...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143317</th>\n",
       "      <td>143317</td>\n",
       "      <td>good product quality price muraahh cepeett del...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74709</th>\n",
       "      <td>74709</td>\n",
       "      <td>the product quality is excellent the original ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33017</th>\n",
       "      <td>33017</td>\n",
       "      <td>fairly cheap though photocopying delivery is a...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144240</th>\n",
       "      <td>144240</td>\n",
       "      <td>fall bosko</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19146</th>\n",
       "      <td>19146</td>\n",
       "      <td>order now 9 baht products to meet the demand o...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124528</th>\n",
       "      <td>124528</td>\n",
       "      <td>product quality mah ga ya need to be asked aga...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80780</th>\n",
       "      <td>80780</td>\n",
       "      <td>alhamdulillah already the dam until the goods ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        review_id                                             review  rating\n",
       "137165     137165  very good value for money like most districts ...       5\n",
       "30633       30633  good product quality product quality good pric...       3\n",
       "118020     118020  packaging containers once with bubble wrap fas...       5\n",
       "143317     143317  good product quality price muraahh cepeett del...       5\n",
       "74709       74709  the product quality is excellent the original ...       4\n",
       "33017       33017  fairly cheap though photocopying delivery is a...       3\n",
       "144240     144240                                         fall bosko       5\n",
       "19146       19146  order now 9 baht products to meet the demand o...       2\n",
       "124528     124528  product quality mah ga ya need to be asked aga...       5\n",
       "80780       80780  alhamdulillah already the dam until the goods ...       4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_df = pd.read_csv(f'{cleanedsrc}train_sol2415.csv')\n",
    "_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = _df.copy()\n",
    "df.review = df.review.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform data and labels into machine-recognizable data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ga disappointed neat products meletot hilsnyaa...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>rdtanya replace broken glass broken chargernya</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>nyesel bngt dsni shopping antecedent photo mes...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>sent a light blue suit goods ga want a refund</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>pendants came with dents and scratches on its ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146806</th>\n",
       "      <td>146806</td>\n",
       "      <td>excellent product quality delivery speed is ve...</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146807</th>\n",
       "      <td>146807</td>\n",
       "      <td>thanks gan</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146808</th>\n",
       "      <td>146808</td>\n",
       "      <td>awesome awesome quality merchandise value cp v...</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146809</th>\n",
       "      <td>146809</td>\n",
       "      <td>nice packing boxes made effective price fast s...</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146810</th>\n",
       "      <td>146810</td>\n",
       "      <td>excellent product quality excellent product pr...</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>146811 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        review_id                                             review  rating  \\\n",
       "0               0  ga disappointed neat products meletot hilsnyaa...       1   \n",
       "1               1     rdtanya replace broken glass broken chargernya       1   \n",
       "2               2  nyesel bngt dsni shopping antecedent photo mes...       1   \n",
       "3               3      sent a light blue suit goods ga want a refund       1   \n",
       "4               4  pendants came with dents and scratches on its ...       1   \n",
       "...           ...                                                ...     ...   \n",
       "146806     146806  excellent product quality delivery speed is ve...       5   \n",
       "146807     146807                                         thanks gan       5   \n",
       "146808     146808  awesome awesome quality merchandise value cp v...       5   \n",
       "146809     146809  nice packing boxes made effective price fast s...       5   \n",
       "146810     146810  excellent product quality excellent product pr...       5   \n",
       "\n",
       "        label  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           0  \n",
       "...       ...  \n",
       "146806      2  \n",
       "146807      2  \n",
       "146808      2  \n",
       "146809      2  \n",
       "146810      2  \n",
       "\n",
       "[146811 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rating to index\n",
    "df['label'] = [0]*len(df)\n",
    "df.loc[df[df['rating'] == 4].index, 'label'] = 1\n",
    "df.loc[df[df['rating'] == 5].index, 'label'] = 2\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ga disappointed neat products meletot hilsnyaa speed of delivery is good', 'rdtanya replace broken glass broken chargernya', 'nyesel bngt dsni shopping antecedent photo message pictures gk according fotodi existing collagen super fit nyampe holo my house open ehh collagen contents even in the face pdahal jg description super existing collagen originalnyapas writing my check lg in photo captions already ma the change ma pictures that the face', 'sent a light blue suit goods ga want a refund', 'pendants came with dents and scratches on its surface the coating looks like it will change colour quickly']\n",
      "[0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "texts = df['review'].tolist()\n",
    "labels = df['label'].tolist()\n",
    "print(texts[:5])\n",
    "print(labels[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random partial data selection\n",
    "<span style=\"color:#FF0000\"><i class=\"fa fa-exclamation-circle\"></i>\n",
    " To fit the whole data, please skip this cell</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_texts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-b71577ff4c7b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtext_arr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_texts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mlabel_arr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# 隨機挑4萬筆出來訓練\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mrandom_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m40000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_texts' is not defined"
     ]
    }
   ],
   "source": [
    "text_arr = np.array(train_texts)\n",
    "label_arr = np.array(train_labels)\n",
    "\n",
    "# 隨機挑4萬筆出來訓練\n",
    "random_index = np.random.choice([i for i in range(len(df))], 40000, replace=False)\n",
    "train_texts = text_arr[random_index].tolist()\n",
    "train_labels = label_arr[random_index].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing\n",
    "將資料處理成電腦能識別的資料型態"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vocabulary numbers:  61174\n",
      "Max sequence length:  1541\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "word_sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_id_dict = tokenizer.word_index\n",
    "print('Total vocabulary numbers: ', len(word_id_dict))\n",
    "seq_maxlen = max([len(ws) for ws in word_sequences])\n",
    "print('Max sequence length: ', seq_maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# padding sequences by post method\n",
    "word_sequences = pad_sequences(word_sequences, padding='post', maxlen=INPUT_SEQUENCE_LENGTH)\n",
    "labels = to_categorical(np.asarray(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split train-test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117448\n",
      "29363\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(word_sequences, labels, test_size=0.2)\n",
    "print(len(x_train))\n",
    "print(len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct the embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dict = {}\n",
    "\n",
    "with open('data/lib/glove.6B.100d.txt', encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embedding_dict[word] = coefs\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are totally 38338 unknown words in data.\n"
     ]
    }
   ],
   "source": [
    "unknown_words = []\n",
    "embedding_matrix = np.zeros((len(word_id_dict) + 1, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_id_dict.items():\n",
    "    embedding_vec = embedding_dict.get(word)\n",
    "    if embedding_vec is not None:\n",
    "        embedding_matrix[i] = embedding_vec\n",
    "    else:\n",
    "        unknown_words.append(word)\n",
    "\n",
    "print('There are totally %d unknown words in data.' % len(unknown_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customize our word vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cepet',\n",
       " 'alhamdulillah',\n",
       " 'shopee',\n",
       " 'reallyt',\n",
       " 'udh',\n",
       " 'pesen',\n",
       " 'sukaa',\n",
       " 'nyampe',\n",
       " 'recomended',\n",
       " 'packingnya',\n",
       " 'tebel',\n",
       " 'dateng',\n",
       " 'dapet',\n",
       " 'krn',\n",
       " 'okee',\n",
       " 'nyesel',\n",
       " 'mksh',\n",
       " 'mantapp',\n",
       " 'trimakasih',\n",
       " 'baguuss',\n",
       " 'smoga',\n",
       " 'kirain',\n",
       " 'pokonya',\n",
       " 'mantul',\n",
       " 'mantull',\n",
       " 'bgtt',\n",
       " 'puass',\n",
       " 'sellernya',\n",
       " 'pdhl',\n",
       " 'baikk',\n",
       " 'smpe',\n",
       " 'makasihh',\n",
       " 'cepatt',\n",
       " 'bngt',\n",
       " 'wrna',\n",
       " 'mantab',\n",
       " 'nyaa',\n",
       " 'smpai',\n",
       " 'realpict',\n",
       " 'tetep',\n",
       " 'hrga',\n",
       " 'bangeett',\n",
       " 'baguus',\n",
       " 'mudah2an',\n",
       " 'sempet',\n",
       " 'dehh',\n",
       " 'lamaa',\n",
       " 'okp',\n",
       " 'lagii',\n",
       " 'jga',\n",
       " 'rekomended',\n",
       " 'brang',\n",
       " 'sekalii',\n",
       " 'bangeet',\n",
       " 'doang',\n",
       " 'sampenya',\n",
       " 'dlu',\n",
       " 'lahh',\n",
       " 'facepalming',\n",
       " 'syukaa',\n",
       " 'packingan',\n",
       " 'nerawang',\n",
       " 'smua',\n",
       " 'krna',\n",
       " 'nyampenya',\n",
       " 'hehehe',\n",
       " 'sukak',\n",
       " 'terimakasihh',\n",
       " 'jugaa',\n",
       " 'brng',\n",
       " 'ceaa',\n",
       " 'mksih',\n",
       " 'lucuu',\n",
       " 'dsni',\n",
       " 'gapapa',\n",
       " 'jgn',\n",
       " 'jnt',\n",
       " 'murahh',\n",
       " 'terbaikk',\n",
       " 'aamiin',\n",
       " 'sampaii',\n",
       " 'bener2',\n",
       " 'huhu',\n",
       " 'pke',\n",
       " 'thankyouu',\n",
       " 'syuka',\n",
       " 'sist',\n",
       " 'gaada',\n",
       " 'youu',\n",
       " 'blnja',\n",
       " 'sihh',\n",
       " 'packagingnya',\n",
       " 'thankss',\n",
       " 'whichsoever',\n",
       " 'goodd',\n",
       " 'dech',\n",
       " 'nhé',\n",
       " 'pngiriman',\n",
       " 'skrg',\n",
       " 'pdhal',\n",
       " 'goodvery',\n",
       " 'kyk',\n",
       " 'engga',\n",
       " 'mantaap',\n",
       " 'ngecewain',\n",
       " 'pobud',\n",
       " 'ngak',\n",
       " 'mantep',\n",
       " 'dipake',\n",
       " 'mantaapp',\n",
       " 'mkasih',\n",
       " 'kakk',\n",
       " 'good2',\n",
       " 'cepett',\n",
       " 'datengnya',\n",
       " 'makasii',\n",
       " 'paketan',\n",
       " 'bnyk',\n",
       " 'anak2',\n",
       " 'kasihh',\n",
       " 'pesennya',\n",
       " 'syg',\n",
       " 'hahaha',\n",
       " 'pengirimanya',\n",
       " 'blanja',\n",
       " 'sampee',\n",
       " 'segini',\n",
       " 'agk',\n",
       " 'recomend',\n",
       " 'dtng',\n",
       " 'cpet',\n",
       " 'rapii',\n",
       " 'nyampek',\n",
       " 'hihi',\n",
       " 'murmer',\n",
       " 'sangatt',\n",
       " 'gatau',\n",
       " 'trnyata',\n",
       " 'olshop',\n",
       " 'bener',\n",
       " 'kecewaa',\n",
       " 'dsini',\n",
       " 'sukakk',\n",
       " 'brgnya',\n",
       " 'pokoknyaa',\n",
       " 'lgsg',\n",
       " 'kerenn',\n",
       " 'kepake',\n",
       " 'tqvm',\n",
       " 'gmn',\n",
       " 'sesuaii',\n",
       " 'wkwk',\n",
       " 'markotop',\n",
       " 'yahh',\n",
       " 'sampek',\n",
       " 'makasiih',\n",
       " 'hargaa',\n",
       " 'gabisa',\n",
       " 'apa2',\n",
       " 'ksih',\n",
       " 'smga',\n",
       " 'cantikk',\n",
       " 'cepat',\n",
       " 'shoppee',\n",
       " 'barangnyaa',\n",
       " 'mantap',\n",
       " 'teruss',\n",
       " 'maganda',\n",
       " 'sellerr',\n",
       " 'nnti',\n",
       " 'ssuai',\n",
       " 'bubblewrap',\n",
       " 'bnyak',\n",
       " 'disinii',\n",
       " '讚讚',\n",
       " 'banyakk',\n",
       " 'makasiihh',\n",
       " 'pakek',\n",
       " 'tipiss',\n",
       " 'dahh',\n",
       " 'mantuull',\n",
       " 'lbih',\n",
       " 'sngat',\n",
       " 'tapii',\n",
       " 'ngepas',\n",
       " 'bhn',\n",
       " 'semuaa',\n",
       " 'disini',\n",
       " 'xde',\n",
       " 'covid',\n",
       " 'talaga',\n",
       " 'kmrn',\n",
       " 'produkk',\n",
       " 'terimakasih',\n",
       " 'hati2',\n",
       " 'packingannya',\n",
       " 'godbless',\n",
       " 'aprox',\n",
       " 'sampai',\n",
       " 'gmbr',\n",
       " 'jaitan',\n",
       " 'dngn',\n",
       " 'ndak',\n",
       " 'rapihh',\n",
       " 'kereen',\n",
       " 'kereenn',\n",
       " 'seler',\n",
       " 'sllu',\n",
       " 'nggk',\n",
       " 'cucok',\n",
       " 'dichat',\n",
       " 'taunya',\n",
       " 'dikasi',\n",
       " 'inii',\n",
       " 'makasih',\n",
       " 'pdahal',\n",
       " 'poslaju',\n",
       " 'sangat2',\n",
       " 'sizenya',\n",
       " 'semogaa',\n",
       " 'paketannya',\n",
       " 'pokoke',\n",
       " 'superr',\n",
       " 'kebuka',\n",
       " 'sampe',\n",
       " 'enakk',\n",
       " 'ilang',\n",
       " 'dalemnya',\n",
       " 'malem',\n",
       " 'bgd',\n",
       " 'respon',\n",
       " 'ecq',\n",
       " 'toopp',\n",
       " 'cepaat',\n",
       " 'psen',\n",
       " 'klw',\n",
       " 'gmna',\n",
       " 'barangx',\n",
       " 'sdah',\n",
       " 'dipakenya',\n",
       " 'wrapnya',\n",
       " 'trmksh',\n",
       " 'insyaallah',\n",
       " 'ggwp',\n",
       " 'recommenda',\n",
       " 'pengiriman',\n",
       " 'psbb',\n",
       " 'sdikit',\n",
       " 'puaass',\n",
       " 'nadeliver',\n",
       " 'dtang',\n",
       " 'cntik',\n",
       " 'maksih',\n",
       " 'koq',\n",
       " 'jugak',\n",
       " 'nihh',\n",
       " 'mgkn',\n",
       " 'bget',\n",
       " 'ademm',\n",
       " 'pokonyaa',\n",
       " 'moneygood',\n",
       " 'dusnya',\n",
       " 'pokoknya',\n",
       " '3days',\n",
       " 'yng',\n",
       " 'barangg',\n",
       " 'lmbt',\n",
       " 'oclock',\n",
       " 'gercep',\n",
       " 'harga',\n",
       " 'sekali',\n",
       " 'bbrp',\n",
       " 'sama2',\n",
       " 'komplen',\n",
       " 'mantuul',\n",
       " 'ehh',\n",
       " 'balikin',\n",
       " 'bwt',\n",
       " 'penyok2',\n",
       " 'nympe',\n",
       " 'awett',\n",
       " 'cpat',\n",
       " 'nyobain',\n",
       " 'akuu',\n",
       " 'bngtt',\n",
       " 'trima',\n",
       " 'cepaatt',\n",
       " 'inshaallah',\n",
       " 'tengkyuu',\n",
       " 'terimaksih',\n",
       " 'langganan',\n",
       " 'nareceive',\n",
       " 'disni',\n",
       " 'pemgiriman',\n",
       " 'sngt',\n",
       " 'casenya',\n",
       " 'heheh',\n",
       " 'originall',\n",
       " 'ancur',\n",
       " 'ajaa',\n",
       " 'pnjual',\n",
       " 'pulak',\n",
       " 'tnx',\n",
       " 'sdkit',\n",
       " 'ownernya',\n",
       " 'laah',\n",
       " 'sering2',\n",
       " 'ramahh',\n",
       " 'gosend',\n",
       " 'yaudah',\n",
       " 'wkwkwk',\n",
       " 'dteng',\n",
       " 'prodak',\n",
       " 'abu2',\n",
       " 'ninjavan',\n",
       " 'bangat',\n",
       " 'kyak',\n",
       " 'produk',\n",
       " 'flashsale',\n",
       " 'nè',\n",
       " 'murah',\n",
       " 'mantabb',\n",
       " 'sukaakk',\n",
       " 'nicee',\n",
       " 'goodsvery',\n",
       " 'laahh',\n",
       " 'gapernah',\n",
       " '2pcs',\n",
       " 'barangny',\n",
       " 'nyangka',\n",
       " 'terimaa',\n",
       " 'murce',\n",
       " 'youthank',\n",
       " '1pcs',\n",
       " 'sesuai',\n",
       " 'klau',\n",
       " 'sukaak',\n",
       " 'lucu2',\n",
       " 'kasiihh',\n",
       " 'selaluu',\n",
       " 'alhamdulillaah',\n",
       " 'syang',\n",
       " 'dongg',\n",
       " 'ongkirnya',\n",
       " 'tengkyu',\n",
       " 'nyampai',\n",
       " 'berkali2',\n",
       " 'bangt',\n",
       " 'puas',\n",
       " 'huhuhu',\n",
       " 'lumayann',\n",
       " 'hehehehe',\n",
       " 'selamatt',\n",
       " 'tdak',\n",
       " 'mnta',\n",
       " 'kcewa',\n",
       " 'dibadan',\n",
       " 'kmren',\n",
       " 'mejo',\n",
       " 'muraah',\n",
       " 'muraahh',\n",
       " 'tuker',\n",
       " 'barng',\n",
       " 'bublewrap',\n",
       " 'gaes',\n",
       " 'kecill',\n",
       " 'sudh',\n",
       " 'pkoknya',\n",
       " 'alhmdulillah',\n",
       " 'thanx',\n",
       " 'deeh',\n",
       " 'cantik',\n",
       " 'nyampee',\n",
       " 'luvv',\n",
       " 'emng',\n",
       " 'pelapak',\n",
       " 'nmn',\n",
       " 'rmh',\n",
       " 'mudah2n',\n",
       " 'alhamdulilah',\n",
       " 'customerku',\n",
       " 'okros',\n",
       " 'ampe',\n",
       " 'jaitannya',\n",
       " 'semoga',\n",
       " 'lumyan',\n",
       " 'nhìu',\n",
       " 'hargalah',\n",
       " 'approbatory',\n",
       " 'sehari2',\n",
       " 'kenceng',\n",
       " 'powerbank',\n",
       " 'deket',\n",
       " 'mlh',\n",
       " 'bhannya',\n",
       " 'parahh',\n",
       " 'nyari',\n",
       " 'rekomend',\n",
       " 'orderan',\n",
       " 'lmyan',\n",
       " 'baikproduk',\n",
       " 'segitu',\n",
       " 'betull',\n",
       " 'terimakasiihh',\n",
       " 'hrus',\n",
       " 'drpd',\n",
       " 'dipesen',\n",
       " 'pesanan',\n",
       " 'skrng',\n",
       " 'pengirimannya',\n",
       " 'lngsung',\n",
       " 'pengirimn',\n",
       " 'memuaskan',\n",
       " 'thnks',\n",
       " 'laen',\n",
       " 'dkrim',\n",
       " 'nagrereply',\n",
       " 'ntar',\n",
       " 'ttep',\n",
       " 'ckup',\n",
       " 'dibales',\n",
       " 'oklah',\n",
       " 'segituu',\n",
       " 'kualitass',\n",
       " 'goodthe',\n",
       " 'orii',\n",
       " 'terimakasiih',\n",
       " 'orderr',\n",
       " 'kasihterima',\n",
       " 'qualitygood',\n",
       " 'bner',\n",
       " 'msih',\n",
       " 'mngkin',\n",
       " 'belii',\n",
       " 'gaess',\n",
       " 'maap',\n",
       " 'brngnya',\n",
       " 'realpic',\n",
       " 'pulaa',\n",
       " 'dkrm',\n",
       " 'knpa',\n",
       " 'sangaatt',\n",
       " 'dtgnya',\n",
       " 'terbaik',\n",
       " 'seneng',\n",
       " 'lmyn',\n",
       " 'dpet',\n",
       " 'baikrespon',\n",
       " 'banyakin',\n",
       " 'cepatbarang',\n",
       " 'tebell',\n",
       " 'cepeet',\n",
       " 'mantepp',\n",
       " 'lmbat',\n",
       " 'dikasihnya',\n",
       " 'hndi',\n",
       " 'semua',\n",
       " 'pakenya',\n",
       " 'nyesell',\n",
       " 'rapet',\n",
       " 'mudah2',\n",
       " 'jkt',\n",
       " 'sayaa',\n",
       " 'pesanann',\n",
       " 'rgaa',\n",
       " 'cutee',\n",
       " 'terimakasii',\n",
       " 'bestt',\n",
       " 'baik2',\n",
       " 'recomen',\n",
       " 'dapetnya',\n",
       " 'sprti',\n",
       " 'shoope',\n",
       " 'trnyta',\n",
       " 'beda2',\n",
       " 'unboxing',\n",
       " 'wrnanya',\n",
       " 'emang',\n",
       " 'iship',\n",
       " 'smuanya',\n",
       " 'pakingnya',\n",
       " 'dipakek',\n",
       " 'pushpin',\n",
       " 'originalharga',\n",
       " 'creamnya',\n",
       " 'keterima',\n",
       " 'jgk',\n",
       " 'kecil2',\n",
       " 'haluss',\n",
       " 'ntah',\n",
       " 'kecewa',\n",
       " 'smpi',\n",
       " 'adminnya',\n",
       " 'sgtu',\n",
       " 'cman',\n",
       " 'gedean',\n",
       " 'memuaskann',\n",
       " 'ganti2',\n",
       " 'siih',\n",
       " 'cocok',\n",
       " 'baikpengiriman',\n",
       " 'bossku',\n",
       " 'syukron',\n",
       " 'sukkaa',\n",
       " 'amaann',\n",
       " 'dpat',\n",
       " 'benar2',\n",
       " 'nman',\n",
       " 'ccok',\n",
       " 'barang2',\n",
       " 'ajh',\n",
       " '4pcs',\n",
       " 'slamat',\n",
       " 'gojek',\n",
       " 'daleman',\n",
       " 'nyoba',\n",
       " 'mukenanya',\n",
       " 'sangaat',\n",
       " 'mantaplah',\n",
       " 'rapi',\n",
       " 'selalu',\n",
       " 'gandaa',\n",
       " 'dineliver',\n",
       " 'jahitanya',\n",
       " 'skli',\n",
       " 'gapapalah',\n",
       " 'pastu',\n",
       " 'kcwa',\n",
       " 'nagsend',\n",
       " 'pengriman',\n",
       " 'pengirimanx',\n",
       " 'thks',\n",
       " 'cocokk',\n",
       " 'mantaff',\n",
       " 'cepeett',\n",
       " 'syukak',\n",
       " 'hpnya',\n",
       " 'bajunyaa',\n",
       " 'kainya',\n",
       " 'redmi',\n",
       " 'mgnda',\n",
       " 'bkin',\n",
       " 'dumating',\n",
       " 'dicobain',\n",
       " 'chatnya',\n",
       " 'mgkin',\n",
       " 'prnh',\n",
       " 'psanan',\n",
       " 'langganann',\n",
       " 'pengirimann',\n",
       " 'yuhuu',\n",
       " 'lumayaan',\n",
       " 'busui',\n",
       " 'terima',\n",
       " 'syaa',\n",
       " 'lngganan',\n",
       " 'sudahh',\n",
       " 'baaik',\n",
       " 'abiss',\n",
       " 'cantiik',\n",
       " 'unyu',\n",
       " 'pkonya',\n",
       " 'lovee',\n",
       " 'puasselalu',\n",
       " 'woww',\n",
       " 'ideliver',\n",
       " 'dibalikin',\n",
       " 'kog',\n",
       " '5star',\n",
       " 'wahh',\n",
       " 'gara2',\n",
       " 'mslh',\n",
       " '3pcs',\n",
       " 'maam',\n",
       " 'kasian',\n",
       " 'batrenya',\n",
       " 'lecet2',\n",
       " 'euy',\n",
       " 'kyknya',\n",
       " 'baikkecepatan',\n",
       " 'sameday',\n",
       " 'yaahh',\n",
       " 'tumben',\n",
       " 'trimaksih',\n",
       " 'aslii',\n",
       " 'kualitas',\n",
       " 'deehh',\n",
       " 'baiik',\n",
       " 'syuukaa',\n",
       " 'banyaakk',\n",
       " 'ramaah',\n",
       " 'gandoss',\n",
       " 'muchh',\n",
       " 'belnja',\n",
       " 'diliat',\n",
       " 'dikirm',\n",
       " 'dikirimin',\n",
       " 'đk',\n",
       " 'pengemasanya',\n",
       " 'pengirman',\n",
       " 'reapon',\n",
       " 'sbnrnya',\n",
       " 'pkai',\n",
       " 'siihh',\n",
       " 'coksu',\n",
       " 'bingit',\n",
       " 'sgitu',\n",
       " 'chgaa',\n",
       " 'ㄧ',\n",
       " 'kcil',\n",
       " 'hnya',\n",
       " 'mukena',\n",
       " 'goodan',\n",
       " 'ulit',\n",
       " 'kasiih',\n",
       " 'kliatan',\n",
       " 'rapiih',\n",
       " 'cgaa',\n",
       " 'enak',\n",
       " 'trimakasihh',\n",
       " 'alhmdllh',\n",
       " 'kaak',\n",
       " 'sampaaii',\n",
       " 'enaak',\n",
       " 'lama2',\n",
       " 'mengecewakan',\n",
       " 'memuaskn',\n",
       " '5pcs',\n",
       " 'batalin',\n",
       " 'kalii',\n",
       " 'briliant',\n",
       " 'sablonan',\n",
       " 'gimna',\n",
       " 'barakallah',\n",
       " 'gambarr',\n",
       " 'deliverygood',\n",
       " 'heheheh',\n",
       " 'nagdeliver',\n",
       " 'sesuailah',\n",
       " 'pedes',\n",
       " 'mkasihh',\n",
       " 'cakepp',\n",
       " 'baaguus',\n",
       " 'akhirnyaa',\n",
       " 'toptop',\n",
       " 'mantaabb',\n",
       " 'yeayy',\n",
       " 'voichat',\n",
       " 'kumplit',\n",
       " '1nya',\n",
       " 'sleting',\n",
       " 'masangnya',\n",
       " 'produknyaa',\n",
       " 'shoopee',\n",
       " 'psenan',\n",
       " 'wlpn',\n",
       " 'expirednya',\n",
       " 'trlalu',\n",
       " 'gituu',\n",
       " 'psan',\n",
       " 'ngelupas',\n",
       " 'soalny',\n",
       " 'pingin',\n",
       " 'barangy',\n",
       " 'shopeehaul',\n",
       " 'kmrin',\n",
       " 'pengeriman',\n",
       " 'okelahh',\n",
       " 'produck',\n",
       " 'dkit',\n",
       " 'kece',\n",
       " 'syekalii',\n",
       " 'mdh2an',\n",
       " 'suksess',\n",
       " 'mimin',\n",
       " 'pengirimannyaa',\n",
       " 'kakaa',\n",
       " 'baagguuss',\n",
       " 'pokokny',\n",
       " 'dengann',\n",
       " 'kapan2',\n",
       " 'sampenyaa',\n",
       " 'sinii',\n",
       " 'wangii',\n",
       " 'hiyaa',\n",
       " 'sumpahh',\n",
       " 'bilangnya',\n",
       " 'betul2',\n",
       " 'terimkasih',\n",
       " 'kalinyaa',\n",
       " 'sakin',\n",
       " '10pcs',\n",
       " 'lumayan',\n",
       " 'tlong',\n",
       " 'pngemasan',\n",
       " 'yaudahlah',\n",
       " 'nhg',\n",
       " 'hahahaha',\n",
       " 'gilaa',\n",
       " 'realpick',\n",
       " 'wlang',\n",
       " 'shopie',\n",
       " '1minggu',\n",
       " 'purchasenya',\n",
       " 'yess',\n",
       " 'moga2',\n",
       " 'tshirt',\n",
       " 'packingny',\n",
       " 'alhamdullilah',\n",
       " 'sicepat',\n",
       " 'wolfis',\n",
       " 'syng',\n",
       " 'peyok',\n",
       " 'cantiikk',\n",
       " 'boxnya',\n",
       " 'lembutt',\n",
       " 'pngriman',\n",
       " 'freegift',\n",
       " 'puuaass',\n",
       " 'puaas',\n",
       " 'mantapkeren',\n",
       " 'kmaren',\n",
       " 'ituu',\n",
       " 'pesann',\n",
       " 'ngecas',\n",
       " 'kecik',\n",
       " 'tgk',\n",
       " 'pjg',\n",
       " 'naorder',\n",
       " 'bajux',\n",
       " 'padin',\n",
       " '1pc',\n",
       " 'bubblenya',\n",
       " 'sblum',\n",
       " 'dikirain',\n",
       " 'barangnya',\n",
       " 'packingx',\n",
       " 'huhuu',\n",
       " 'custumer',\n",
       " 'ngg',\n",
       " 'hrganya',\n",
       " 'yaah',\n",
       " 'ternyta',\n",
       " 'mintak',\n",
       " 'nuhun',\n",
       " 'dipakein',\n",
       " 'harganyaa',\n",
       " 'nyampeknya',\n",
       " 'sndiri',\n",
       " 'bhnnya',\n",
       " 'bahannyaa',\n",
       " 'kemana2',\n",
       " '2days',\n",
       " 'trimksh',\n",
       " 'produkny',\n",
       " 'hehee',\n",
       " 'pnting',\n",
       " 'aminn',\n",
       " 'baiikk',\n",
       " 'baaguuss',\n",
       " 'alhmdulilah',\n",
       " 'warnanyaa',\n",
       " 'ngay',\n",
       " 'bbaagguuss',\n",
       " 'sulit',\n",
       " 'shopp',\n",
       " 'moneyfast',\n",
       " 'biasaa',\n",
       " 'berbaloi',\n",
       " 'ᴗ',\n",
       " 'kurg',\n",
       " 'dipengiriman',\n",
       " 'nareceived',\n",
       " 'bkal',\n",
       " 'khimars',\n",
       " 'gamau',\n",
       " 'thnx',\n",
       " 'jwb',\n",
       " 'nagana',\n",
       " 'mhn',\n",
       " 'gausah',\n",
       " 'asal2an',\n",
       " 'mantaf',\n",
       " 'airpods',\n",
       " 'adminya',\n",
       " 'tmbah',\n",
       " 'pkok',\n",
       " 'khimarnya',\n",
       " 'wardah',\n",
       " 'lgsung',\n",
       " 'mngecewakan',\n",
       " 'mganda',\n",
       " 'bjunya',\n",
       " 'belanjaa',\n",
       " 'barokah',\n",
       " 'bahanny',\n",
       " 'samapai',\n",
       " 'teball',\n",
       " '5sao',\n",
       " 'agad',\n",
       " 'sendalnya',\n",
       " 'hhe',\n",
       " 'lumayaann',\n",
       " 'murcee',\n",
       " 'mntap',\n",
       " 'baraang',\n",
       " 'naikin',\n",
       " 'harha',\n",
       " 'cantik2',\n",
       " 'anaku',\n",
       " 'thnk',\n",
       " 'rapiihh',\n",
       " 'syuukkaa',\n",
       " 'sokp',\n",
       " 'mksi',\n",
       " 'terimakasi',\n",
       " 'terjangkauu',\n",
       " 'banyaak',\n",
       " 'amiin',\n",
       " 'maniss',\n",
       " 'nhaa',\n",
       " 'tqq',\n",
       " 'nhanhh',\n",
       " 'exelent',\n",
       " 'lemes',\n",
       " 'kerdus',\n",
       " 'orng',\n",
       " 'lohh',\n",
       " 'rereply',\n",
       " 'kurangg',\n",
       " 'responn',\n",
       " 'pakaging',\n",
       " 'kagak',\n",
       " 'guyss',\n",
       " 'expedisinya',\n",
       " 'batrai',\n",
       " 'mkn',\n",
       " 'masing2',\n",
       " 'expetasi',\n",
       " 'baikkualitas',\n",
       " 'trimakasi',\n",
       " 'bahanx',\n",
       " 'expektasi',\n",
       " 'sangt',\n",
       " 'cumn',\n",
       " '4hari',\n",
       " 'bgtu',\n",
       " 'tpii',\n",
       " '6pcs',\n",
       " 'pakein',\n",
       " 'pngirimannya',\n",
       " 'jdnya',\n",
       " 'ngatung',\n",
       " 'hemm',\n",
       " 'cakeepp',\n",
       " 'adaa',\n",
       " 'syekali',\n",
       " 'ใcha',\n",
       " 'murah2',\n",
       " 'fotoin',\n",
       " 'bahannya',\n",
       " 'gpplah',\n",
       " 'mantaps',\n",
       " 'keliatannya',\n",
       " 'aweet',\n",
       " 'bgss',\n",
       " 'trimksih',\n",
       " 'yeay',\n",
       " 'selleerr',\n",
       " 'keycap',\n",
       " 'yuhu',\n",
       " 'kecilin',\n",
       " 'kka',\n",
       " 'ookk',\n",
       " 'mumer',\n",
       " 'markotopp',\n",
       " 'reccomended',\n",
       " 'sukasuka',\n",
       " 'hbis',\n",
       " 'bkan',\n",
       " '3xl',\n",
       " 'wkwkw',\n",
       " 'bleh',\n",
       " 'dpke',\n",
       " 'khimar',\n",
       " 'luon',\n",
       " 'tiba2',\n",
       " 'benang2',\n",
       " 'aahh',\n",
       " 'kainx',\n",
       " 'pokokx',\n",
       " 'recomanded',\n",
       " 'tidk',\n",
       " 'cug',\n",
       " 'malowbat',\n",
       " 'mesennya',\n",
       " 'buanget',\n",
       " 'satu2',\n",
       " 'sobrang',\n",
       " 'wlaupun',\n",
       " 'sdkt',\n",
       " 'krng',\n",
       " 'baget',\n",
       " 'besok2',\n",
       " 'natry',\n",
       " 'amanahh',\n",
       " 'baikterima',\n",
       " 'bahany',\n",
       " 'pengirimaan',\n",
       " 'mdh2n',\n",
       " 'selamaat',\n",
       " 'ngabarin',\n",
       " 'adain',\n",
       " 'sudah',\n",
       " 'hatur',\n",
       " 'suukkaa',\n",
       " 'dikantong',\n",
       " 'yeyy',\n",
       " 'allhamdulillah',\n",
       " 'enaakk',\n",
       " 'kembaliin',\n",
       " 'penyot',\n",
       " 'dgan',\n",
       " 'kecwa',\n",
       " 'barangya',\n",
       " 'kejahit',\n",
       " 'dua2nya',\n",
       " 'hadehh',\n",
       " 'dikrm',\n",
       " 'luamaa',\n",
       " 'makenya',\n",
       " 'tumpah2',\n",
       " 'gamis',\n",
       " 'pantes',\n",
       " 'kyanya',\n",
       " 'jngn',\n",
       " 'yey',\n",
       " 'baramg',\n",
       " 'gedee',\n",
       " 'bntang',\n",
       " 'ngefek',\n",
       " 'dikirim',\n",
       " 'bgian',\n",
       " 'rùi',\n",
       " 'dapat',\n",
       " 'incrustation',\n",
       " 'orderannya',\n",
       " 'ktnya',\n",
       " 'mkasi',\n",
       " 'buru2',\n",
       " 'furing',\n",
       " 'goodpengiriman',\n",
       " 'hny',\n",
       " 'dasternya',\n",
       " 'wlpun',\n",
       " 'imut2',\n",
       " 'naship',\n",
       " 'ㄉ',\n",
       " 'hehehee',\n",
       " 'okehh',\n",
       " 'sesui',\n",
       " 'alhamdulillahh',\n",
       " 'kokk',\n",
       " 'pertahanin',\n",
       " 'reallyvery',\n",
       " 'mantabs',\n",
       " 'cepatrespon',\n",
       " 'kakak',\n",
       " 'ramaahh',\n",
       " 'sellertq',\n",
       " 'meriahh',\n",
       " 'cucookk',\n",
       " 'daahh',\n",
       " 'trimakasihlumayan',\n",
       " 'againthank',\n",
       " 'cakeep',\n",
       " 'langangan',\n",
       " 'jsjsjsjjdhdjsjkskd',\n",
       " 'dibatalin',\n",
       " 'psnan',\n",
       " 'bnr2',\n",
       " 'tipiiss',\n",
       " 'pecahh',\n",
       " 'samsek',\n",
       " 'berkah',\n",
       " 'kira2',\n",
       " 'dissappointed',\n",
       " 'pnya',\n",
       " 'aduhh',\n",
       " 'yaampun',\n",
       " 'retur',\n",
       " 'bner2',\n",
       " 'kebaca',\n",
       " 'softcase',\n",
       " 'okok',\n",
       " 'dikirim2',\n",
       " 'ukuranya',\n",
       " 'byr',\n",
       " 'jdinya',\n",
       " 'boong',\n",
       " ...]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unknown_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import multiprocessing\n",
    "\n",
    "word2vec_model = Word2Vec(sentences=unknown_words, \n",
    "                 sg=1, \n",
    "                 size=100, \n",
    "                 min_count=1,\n",
    "                 workers=multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "358"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2vec_model.wv.vocab.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到只有1116個未知詞彙嵌入了個vector  \n",
    "目前還在解決大量未知詞彙無法嵌入vector的問題，只好把這些未知詞彙跟著模型一起訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(input_dim=len(word_id_dict)+1, \n",
    "                           output_dim=EMBEDDING_DIM, \n",
    "                           weights=[embedding_matrix], \n",
    "                           input_length=MAX_VOCABULARY_NUM, \n",
    "                           trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 1600)]            0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 1600, 100)         6117500   \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 1596, 256)         128256    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 797, 256)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 797, 256)          3188      \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 793, 256)          327936    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 396, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 392, 256)          327936    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 195, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 191, 256)          327936    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 63, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 59, 256)           327936    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 771       \n",
      "=================================================================\n",
      "Total params: 7,627,251\n",
      "Trainable params: 7,625,657\n",
      "Non-trainable params: 1,594\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# increase the filters amount and layers amount\n",
    "# use adam optimizer\n",
    "# change the activation function of classify layer to sigmoid function\n",
    "seq_input = Input(shape=(INPUT_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedding_seq = embedding_layer(seq_input)\n",
    "\n",
    "conv_layer1 = Conv1D(256, 5, activation='relu')(embedding_seq)\n",
    "pool_layer1 = MaxPooling1D(pool_size=3, strides=2)(conv_layer1)\n",
    "normal_layer = BatchNormalization(axis=1, epsilon=0.0001)(pool_layer1)\n",
    "\n",
    "conv_layer2 = Conv1D(256, 5, activation='relu')(normal_layer)\n",
    "pool_layer2 = MaxPooling1D(pool_size=3, strides=2)(conv_layer2)\n",
    "\n",
    "conv_layer3 = Conv1D(256, 5, activation='relu')(pool_layer2)\n",
    "pool_layer3 = MaxPooling1D(pool_size=3, strides=2)(conv_layer3)\n",
    "\n",
    "conv_layer4 = Conv1D(256, 5, activation='relu')(pool_layer3)\n",
    "pool_layer4 = MaxPooling1D(pool_size=3)(conv_layer4)\n",
    "\n",
    "conv_layer5 = Conv1D(256, 5, activation='relu')(pool_layer4)\n",
    "gpool_layer = GlobalMaxPooling1D()(conv_layer5)\n",
    "\n",
    "drop_layer1 = Dropout(.1)(gpool_layer)\n",
    "\n",
    "flatten_layer = Flatten()(drop_layer1)\n",
    "dense_layer = Dense(256, activation='relu')(flatten_layer)\n",
    "drop_layer2 = Dropout(.1)(dense_layer)\n",
    "predict_layer = Dense(LABEL_NUM, activation='softmax')(drop_layer2)\n",
    "\n",
    "model=Model(seq_input, predict_layer)\n",
    "model.compile(optimizer=keras.optimizers.Adam(lr=0.002),\n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\")])\n",
    "model.summary()\n",
    "cp = ModelCheckpoint('model/model_cnn_test.hdf5',monitor='val_accuracy',verbose=1,save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    C:\\Users\\pb580\\anaconda3\\envs\\ML_Env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:571 train_function  *\n        outputs = self.distribute_strategy.run(\n    C:\\Users\\pb580\\anaconda3\\envs\\ML_Env\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:951 run  **\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\pb580\\anaconda3\\envs\\ML_Env\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2290 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\pb580\\anaconda3\\envs\\ML_Env\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2649 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\pb580\\anaconda3\\envs\\ML_Env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:533 train_step  **\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    C:\\Users\\pb580\\anaconda3\\envs\\ML_Env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\compile_utils.py:205 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    C:\\Users\\pb580\\anaconda3\\envs\\ML_Env\\lib\\site-packages\\tensorflow\\python\\keras\\losses.py:143 __call__\n        losses = self.call(y_true, y_pred)\n    C:\\Users\\pb580\\anaconda3\\envs\\ML_Env\\lib\\site-packages\\tensorflow\\python\\keras\\losses.py:246 call\n        return self.fn(y_true, y_pred, **self._fn_kwargs)\n    C:\\Users\\pb580\\anaconda3\\envs\\ML_Env\\lib\\site-packages\\tensorflow\\python\\keras\\losses.py:1558 sparse_categorical_crossentropy\n        y_true, y_pred, from_logits=from_logits, axis=axis)\n    C:\\Users\\pb580\\anaconda3\\envs\\ML_Env\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py:4655 sparse_categorical_crossentropy\n        labels=target, logits=output)\n    C:\\Users\\pb580\\anaconda3\\envs\\ML_Env\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py:3591 sparse_softmax_cross_entropy_with_logits_v2\n        labels=labels, logits=logits, name=name)\n    C:\\Users\\pb580\\anaconda3\\envs\\ML_Env\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py:3507 sparse_softmax_cross_entropy_with_logits\n        logits.get_shape()))\n\n    ValueError: Shape mismatch: The shape of labels (received (3,)) should equal the shape of logits except for the last dimension (received (1, 3)).\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-cb4c8305db15>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcp\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\ML_Env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ML_Env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 848\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    849\u001b[0m               \u001b[1;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    850\u001b[0m               \u001b[1;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ML_Env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    579\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 580\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    581\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ML_Env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    616\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    617\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 618\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    619\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    620\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ML_Env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2417\u001b[0m     \u001b[1;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2418\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2419\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2420\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ML_Env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   2772\u001b[0m           \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_signature\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2773\u001b[0m           and call_context_key in self._function_cache.missed):\n\u001b[1;32m-> 2774\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_define_function_with_shape_relaxation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2775\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2776\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ML_Env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_define_function_with_shape_relaxation\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   2704\u001b[0m         relaxed_arg_shapes)\n\u001b[0;32m   2705\u001b[0m     graph_function = self._create_graph_function(\n\u001b[1;32m-> 2706\u001b[1;33m         args, kwargs, override_flat_arg_shapes=relaxed_arg_shapes)\n\u001b[0m\u001b[0;32m   2707\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marg_relaxed\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrank_only_cache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2708\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ML_Env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   2665\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2666\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2667\u001b[1;33m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[0;32m   2668\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2669\u001b[0m         \u001b[1;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ML_Env\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    979\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    980\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 981\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    982\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    983\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ML_Env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    439\u001b[0m         \u001b[1;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    440\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 441\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    442\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    443\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ML_Env\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    966\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 968\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    969\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    970\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    C:\\Users\\pb580\\anaconda3\\envs\\ML_Env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:571 train_function  *\n        outputs = self.distribute_strategy.run(\n    C:\\Users\\pb580\\anaconda3\\envs\\ML_Env\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:951 run  **\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\pb580\\anaconda3\\envs\\ML_Env\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2290 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\pb580\\anaconda3\\envs\\ML_Env\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2649 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\pb580\\anaconda3\\envs\\ML_Env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:533 train_step  **\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    C:\\Users\\pb580\\anaconda3\\envs\\ML_Env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\compile_utils.py:205 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    C:\\Users\\pb580\\anaconda3\\envs\\ML_Env\\lib\\site-packages\\tensorflow\\python\\keras\\losses.py:143 __call__\n        losses = self.call(y_true, y_pred)\n    C:\\Users\\pb580\\anaconda3\\envs\\ML_Env\\lib\\site-packages\\tensorflow\\python\\keras\\losses.py:246 call\n        return self.fn(y_true, y_pred, **self._fn_kwargs)\n    C:\\Users\\pb580\\anaconda3\\envs\\ML_Env\\lib\\site-packages\\tensorflow\\python\\keras\\losses.py:1558 sparse_categorical_crossentropy\n        y_true, y_pred, from_logits=from_logits, axis=axis)\n    C:\\Users\\pb580\\anaconda3\\envs\\ML_Env\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py:4655 sparse_categorical_crossentropy\n        labels=target, logits=output)\n    C:\\Users\\pb580\\anaconda3\\envs\\ML_Env\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py:3591 sparse_softmax_cross_entropy_with_logits_v2\n        labels=labels, logits=logits, name=name)\n    C:\\Users\\pb580\\anaconda3\\envs\\ML_Env\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py:3507 sparse_softmax_cross_entropy_with_logits\n        logits.get_shape()))\n\n    ValueError: Shape mismatch: The shape of labels (received (3,)) should equal the shape of logits except for the last dimension (received (1, 3)).\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, epochs=15, validation_data=(x_test, y_test), batch_size=1, callbacks=[cp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_accuracy = history.history['accuracy']\n",
    "h_val_accuracy = history.history['val_accuracy']\n",
    "h_loss = history.history['loss']\n",
    "h_val_loss = history.history['val_loss']\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2)\n",
    "\n",
    "fig.add_trace(go.Scatter(y=h_accuracy, mode='lines+markers', name='accuracy', line=dict(color='skyblue')),\n",
    "              row=1, col=1)\n",
    "fig.add_trace(go.Scatter(y=h_val_accuracy, mode='lines+markers', name='validation accuracy', line=dict(color='dodgerblue')),\n",
    "              row=1, col=1)\n",
    "\n",
    "fig.add_trace(go.Scatter(y=h_loss, mode='lines+markers',name='loss', line=dict(color='lightsalmon')),\n",
    "              row=1, col=2)\n",
    "fig.add_trace(go.Scatter(y=h_val_loss, mode='lines+markers', name='validation loss', line=dict(color='tomato')),\n",
    "              row=1, col=2)\n",
    "\n",
    "fig.update_xaxes(title_text='Epochs', row=1, col=1)\n",
    "fig.update_xaxes(title_text='Epochs', row=1, col=2)\n",
    "fig.update_yaxes(title_text='Accuracy', row=1, col=1)\n",
    "fig.update_yaxes(title_text='Loss', row=1, col=2)\n",
    "\n",
    "fig.update_layout(title='Model Performation', height=480, width=1080)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data prediction\n",
    "I don't know why the test data on kaggle is different with the one on google drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdf = pd.read_csv('data/test.csv')\n",
    "testdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(testdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = testdf['review'].tolist()\n",
    "test_seq = tokenizer.texts_to_sequences(test)\n",
    "test_seq = pad_sequences(test_seq, padding='post', maxlen=WORD_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(test_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.argmax(pred, axis=1)\n",
    "classes = classes + 1\n",
    "submission = testdf.drop('review', axis=1)\n",
    "submission['rating']=classes\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('===========Description===========\\n', submission.describe(), '\\n')\n",
    "print('rating 1: ', submission[submission['rating'] == 1].rating.count())\n",
    "print('rating 2: ', submission[submission['rating'] == 2].rating.count())\n",
    "print('rating 3: ', submission[submission['rating'] == 3].rating.count())\n",
    "print('rating 4: ', submission[submission['rating'] == 4].rating.count())\n",
    "print('rating 5: ', submission[submission['rating'] == 5].rating.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission/submission_00.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
